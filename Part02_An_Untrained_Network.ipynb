{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 2: An Untrained network\nWe're going to start with a basic neural network that is designed to learn how to label colors. Don't worry, you don't need to know how the neural network works yet, but what we're going to learn first is about the need for *training* the network. We're going to start with a blank, untrained network, and see how it does. You could think of this network as a newborn baby who opens its eyes for the first time, not knowing the names of anything yet, but able to receive red, green, and blue color information. We're going to see how the baby does when no one has taught it any names yet.\n\nGo ahead and click Run."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from keras.layers import Activation, Dense, Dropout\nfrom keras.models import Sequential\nimport keras.optimizers, keras.utils, numpy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\ndef train():\n    \"\"\"\n    Creates a blank, untrained neural network.\n    \"\"\"\n\n    # We always need a couple of output neurons. Later we'll use actual colors.\n    numColors = 2\n    colorLabels = [ 'none1', 'none2' ]\n    \n    # Hyperparameters to define the network shape.\n    numFullyConnectedPerceptrons = numColors * 4\n    \n    model = Sequential([\n        # Layer 1: Fully connected layer with ReLU activation.\n        Dense(numFullyConnectedPerceptrons, activation='relu', kernel_initializer='TruncatedNormal', input_shape=(3,)),\n\n        # Outputs: SoftMax activation to get probabilities by color.\n        Dense(numColors, activation='softmax')\n    ])\n\n    print(model.summary())\n\n    # Compile for categorization.\n    model.compile(\n        optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9, decay = 1e-6, nesterov = True),\n        loss = 'categorical_crossentropy',\n        metrics = [ 'accuracy' ])\n\n    return (model, colorLabels)\n    \n(colorModel, colorLabels) = train()\n",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_9 (Dense)              (None, 8)                 32        \n_________________________________________________________________\ndense_10 (Dense)             (None, 2)                 18        \n=================================================================\nTotal params: 50\nTrainable params: 50\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Above you can see a text summary of the shape of the neural network. Ready for a bit of complication? We're using a 1980s-style two-layer network. The first layer uses 4 perceptrons per color, and is connected to a second layer that has one output predictor neuron per color.\n\nDon't worry, we'll learn more about what this means as we go along.\n\nJust like in part 1, we can use RGB color sliders to set a color. But this time we get to see what the neural network predicts for each color. The color names are in one list like:\n\n `['none1', 'none2']`\n \nAnd the next list shows the percentage likelihood of each of those colors being correct, like:\n\n `[49.5 50.5]`\n \nGo ahead and play with the sliders and make whatever colors you want. Keep an eye on the percentages. We'll discuss below."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from IPython.core.display import display, HTML\nfrom ipywidgets import interact\ndef displayColor(r, g, b):\n    rInt = min(255, max(0, int(r * 255.0)))\n    gInt = min(255, max(0, int(g * 255.0)))\n    bInt = min(255, max(0, int(b * 255.0)))\n    hexColor = \"#%02X%02X%02X\" % (rInt, gInt, bInt)\n    display(HTML('<div style=\"width: 50%; height: 50px; background: ' + hexColor + ';\"></div>'))\n\n@interact(r = (0.0, 1.0, 0.01), g = (0.0, 1.0, 0.01), b = (0.0, 1.0, 0.01))\ndef getPredictionsFromModel(r, g, b):\n    testColor = numpy.array([ (r, g, b) ])\n    predictions = colorModel.predict(testColor, verbose=0)  # Predictions shape (1, numColors)\n    predictions *= 100.0\n    print(colorLabels)\n    numpy.set_printoptions(precision=1, suppress=True)\n    print(predictions[0])\n    displayColor(r, g, b)\n",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2482f92d3de4369ad8a7f1c040a2ca2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "interactive(children=(FloatSlider(value=0.5, description='r', max=1.0, step=0.01), FloatSlider(value=0.5, descâ€¦"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you notice that no matter what color you make, the network is predicting the 'none1' and 'none2' colors at right around 50%? This is what an untrained network looks like. It takes the RGB values and runs them through the network, which contains some small random numbers, and when it gets done the predictions hover around 50% which simply means uncertainty.\n\nWhen a network is more certain that it does not see a certain color, you will see numbers closer to 0%. When it's more certain, you'll see numbers above 80%, with high certainty above 95%.\n\nThat's what you get with neural networks: Almost never a big \"Aha!\" with 100% certainty, but just various levels of certainty. It almost feels like the network just doesn't want to commit to anything and maybe be wrong! But in fact it's really that the neural network is a bunch of mathematical equations that through training canlearn the shape of data and get pretty close.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Coming Up...\nIn part 3 we'll actually train this network with a few colors. Hopefully the network will give us some better results..."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}