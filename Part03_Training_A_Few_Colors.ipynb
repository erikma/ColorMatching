{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Part 3: Training a Few Colors\nLet's actually train the basic network with a small list of colors. First, let's copy the network from the previous part, but add in the ability to pass a map from color names to RGB values. Go ahead and click Run to create the train() function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.layers import Activation, Dense, Dropout\nfrom keras.models import Sequential\nimport keras.optimizers, keras.utils, numpy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\ndef train(colorNameToRGBMap, epochs = 16):\n    \"\"\"\n    Trains a neural network to understand how to map color names to RGB triples.\n    The provided map is from 'color-name':(r,g,b) where r,g,b are floats in the range [0,1].\n    Different names are allowed to map to the same RGB triple.\n    Returns a trained model that can be used for recognize().\n    \"\"\"\n\n    # Convert the Python map RGB values into a numpy array needed for training.\n    rgbNumpyArray = numpy.array(list(colorNameToRGBMap.values()), numpy.float)\n    \n    # Convert the color labels into a one-hot feature array.\n    # Text labels for each array position are in the classes_ list on the binarizer.\n    labelBinarizer = LabelBinarizer()\n    oneHotLabels = labelBinarizer.fit_transform(list(colorNameToRGBMap.keys()))\n    numColors = len(labelBinarizer.classes_)\n    colorLabels = labelBinarizer.classes_\n    \n    # Hyperparameters to define the network shape.\n    numFullyConnectedPerceptrons = numColors * 16\n    batchSize = 1\n    \n    model = Sequential([\n        # Layer 1: Fully connected layer with ReLU activation.\n        Dense(numFullyConnectedPerceptrons, activation='relu', kernel_initializer='TruncatedNormal', input_shape=(3,)),\n\n        # Outputs: SoftMax activation to get probabilities by color.\n        Dense(numColors, activation='softmax')\n    ])\n\n    print(model.summary())\n\n    # Compile for categorization.\n    model.compile(\n        optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9, decay = 1e-6, nesterov = True),\n        loss = 'categorical_crossentropy',\n        metrics = [ 'accuracy' ])\n\n    history = model.fit(rgbNumpyArray, oneHotLabels, epochs=epochs, batch_size=batchSize)\n\n    return (model, colorLabels)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "OK, now we have the train() function, let's provide it with a color map with our first three colors, black, gray, and white, and train it. Click Run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "colorMap = {\n    'white': (1.0, 1.0, 1.0),\n    'black': (0.0, 0.0, 0.0),\n    'gray': (0.5, 0.5, 0.5)\n}\n\n(colorModel, colorLabels) = train(colorMap)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_9 (Dense)              (None, 48)                192       \n_________________________________________________________________\ndense_10 (Dense)             (None, 3)                 147       \n=================================================================\nTotal params: 339\nTrainable params: 339\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/16\n3/3 [==============================] - 0s - loss: 1.1084 - acc: 0.0000e+00     \nEpoch 2/16\n3/3 [==============================] - 0s - loss: 1.0866 - acc: 0.0000e+00     \nEpoch 3/16\n3/3 [==============================] - 0s - loss: 1.0652 - acc: 0.3333         \nEpoch 4/16\n3/3 [==============================] - 0s - loss: 1.0509 - acc: 0.3333         \nEpoch 5/16\n3/3 [==============================] - 0s - loss: 1.0300 - acc: 0.3333         \nEpoch 6/16\n3/3 [==============================] - 0s - loss: 1.0111 - acc: 0.6667     \nEpoch 7/16\n3/3 [==============================] - 0s - loss: 0.9951 - acc: 0.6667     \nEpoch 8/16\n3/3 [==============================] - 0s - loss: 0.9686 - acc: 0.6667     \nEpoch 9/16\n3/3 [==============================] - 0s - loss: 0.9510 - acc: 0.6667     \nEpoch 10/16\n3/3 [==============================] - 0s - loss: 0.9293 - acc: 0.6667         \nEpoch 11/16\n3/3 [==============================] - 0s - loss: 0.9142 - acc: 0.6667     \nEpoch 12/16\n3/3 [==============================] - 0s - loss: 0.8906 - acc: 0.6667     \nEpoch 13/16\n3/3 [==============================] - 0s - loss: 0.8640 - acc: 0.6667     \nEpoch 14/16\n3/3 [==============================] - 0s - loss: 0.8491 - acc: 0.6667     \nEpoch 15/16\n3/3 [==============================] - 0s - loss: 0.8319 - acc: 0.6667     \nEpoch 16/16\n3/3 [==============================] - 0s - loss: 0.8081 - acc: 0.6667         \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's reuse the same color slider that runs the neural network and shows percentages."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython.core.display import display, HTML\nfrom ipywidgets import interact\ndef displayColor(r, g, b):\n    rInt = min(255, max(0, int(r * 255.0)))\n    gInt = min(255, max(0, int(g * 255.0)))\n    bInt = min(255, max(0, int(b * 255.0)))\n    hexColor = \"#%02X%02X%02X\" % (rInt, gInt, bInt)\n    display(HTML('<div style=\"width: 50%; height: 50px; background: ' + hexColor + ';\"></div>'))\n\n@interact(r = (0.0, 1.0, 0.01), g = (0.0, 1.0, 0.01), b = (0.0, 1.0, 0.01))\ndef getPredictionsFromModel(r, g, b):\n    testColor = numpy.array([ (r, g, b) ])\n    predictions = colorModel.predict(testColor, verbose=0)  # Predictions shape (1, numColors)\n    predictions *= 100.0\n    print(colorLabels)\n    numpy.set_printoptions(precision=1, suppress=True)\n    print(predictions[0])\n    displayColor(r, g, b)\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "248c239c05c644168b94df8c00443140",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "interactive(children=(FloatSlider(value=0.5, description='r', max=1.0, step=0.01), FloatSlider(value=0.5, desc…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Play with the colors and watch the percentages. Notice how it starts at gray but the network only gives about a 32% likelihood of the color being gray? If you slide the sliders to black you get about a 28% prediction for the label 'black', and if you slide to all white you get a 62% likelihood. Not very good! We're gong to have to find ways to do better.\n\n## Epochs: Like Talking to a Toddler\nLet's continue with our baby analogy from before. We started training by providing examples to the network and letting it start to figure things out. But just like humans, babies and toddlers like a lot of repetition when they are learning. And just like toddlers, repeating usually lets the network learn better each time, at least up to a point. You can control how much you repeat all the examples to the network using a number called the _epoch count_. An epoch runs all the training examples against the network one time. So setting the epoch count to 5 repeats all the data 5 times. The network training system will actually show a number back to you called _loss_ which, if things are going well, should get closer and closer to zero.\n\nIn the training above, we used 16 epochs. Take a look at its output. On my machine the last two training epochs showed:\n\n `Epoch 15/16\n 3/3 [==============================] - 0s - loss: 0.8288 - acc: 0.6667`\n\nand\n\n `Epoch 16/16\n 3/3 [==============================] - 0s - loss: 0.8042 - acc: 0.6667`\n \nOn epoch 15 the loss was about 0.82, while on epoch 16 it was 0.80, so we're getting a bit better each time we repeat the examples, but ideally we'd want to get the loss number down as close to zero as we can.\n\nSo let's repeat ourselves a lot more. Let's use 100 epochs to see what happens:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "(colorModel, colorLabels) = train(colorMap, epochs=100)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_11 (Dense)             (None, 48)                192       \n_________________________________________________________________\ndense_12 (Dense)             (None, 3)                 147       \n=================================================================\nTotal params: 339\nTrainable params: 339\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/100\n3/3 [==============================] - 0s - loss: 1.1000 - acc: 0.3333     \nEpoch 2/100\n3/3 [==============================] - 0s - loss: 1.0751 - acc: 0.0000e+00     \nEpoch 3/100\n3/3 [==============================] - 0s - loss: 1.0649 - acc: 0.3333     \nEpoch 4/100\n3/3 [==============================] - 0s - loss: 1.0422 - acc: 0.3333     \nEpoch 5/100\n3/3 [==============================] - 0s - loss: 1.0171 - acc: 0.3333         \nEpoch 6/100\n3/3 [==============================] - 0s - loss: 0.9967 - acc: 0.3333         \nEpoch 7/100\n3/3 [==============================] - 0s - loss: 0.9798 - acc: 0.3333     \nEpoch 8/100\n3/3 [==============================] - 0s - loss: 0.9529 - acc: 0.6667         \nEpoch 9/100\n3/3 [==============================] - 0s - loss: 0.9353 - acc: 0.6667     \nEpoch 10/100\n3/3 [==============================] - 0s - loss: 0.9202 - acc: 0.6667     \nEpoch 11/100\n3/3 [==============================] - 0s - loss: 0.8931 - acc: 0.6667         \nEpoch 12/100\n3/3 [==============================] - 0s - loss: 0.8712 - acc: 0.6667         \nEpoch 13/100\n3/3 [==============================] - 0s - loss: 0.8561 - acc: 0.6667     \nEpoch 14/100\n3/3 [==============================] - 0s - loss: 0.8321 - acc: 0.6667         \nEpoch 15/100\n3/3 [==============================] - 0s - loss: 0.8165 - acc: 0.6667     \nEpoch 16/100\n3/3 [==============================] - 0s - loss: 0.7964 - acc: 0.6667     \nEpoch 17/100\n3/3 [==============================] - 0s - loss: 0.7758 - acc: 0.6667     \nEpoch 18/100\n3/3 [==============================] - 0s - loss: 0.7593 - acc: 0.6667         \nEpoch 19/100\n3/3 [==============================] - 0s - loss: 0.7457 - acc: 0.6667         \nEpoch 20/100\n3/3 [==============================] - 0s - loss: 0.7232 - acc: 0.6667         \nEpoch 21/100\n3/3 [==============================] - 0s - loss: 0.7094 - acc: 0.6667     \nEpoch 22/100\n3/3 [==============================] - 0s - loss: 0.6988 - acc: 0.6667     \nEpoch 23/100\n3/3 [==============================] - 0s - loss: 0.6798 - acc: 0.6667     \nEpoch 24/100\n3/3 [==============================] - 0s - loss: 0.6686 - acc: 0.6667         \nEpoch 25/100\n3/3 [==============================] - 0s - loss: 0.6479 - acc: 0.6667         \nEpoch 26/100\n3/3 [==============================] - 0s - loss: 0.6343 - acc: 0.6667         \nEpoch 27/100\n3/3 [==============================] - 0s - loss: 0.6233 - acc: 0.6667     \nEpoch 28/100\n3/3 [==============================] - 0s - loss: 0.6111 - acc: 0.6667         \nEpoch 29/100\n3/3 [==============================] - 0s - loss: 0.5958 - acc: 0.6667         \nEpoch 30/100\n3/3 [==============================] - 0s - loss: 0.5951 - acc: 0.6667     \nEpoch 31/100\n3/3 [==============================] - 0s - loss: 0.5784 - acc: 0.6667     \nEpoch 32/100\n3/3 [==============================] - 0s - loss: 0.5663 - acc: 0.6667     \nEpoch 33/100\n3/3 [==============================] - 0s - loss: 0.5597 - acc: 0.6667         \nEpoch 34/100\n3/3 [==============================] - 0s - loss: 0.5478 - acc: 0.6667         \nEpoch 35/100\n3/3 [==============================] - 0s - loss: 0.5420 - acc: 1.0000     \nEpoch 36/100\n3/3 [==============================] - 0s - loss: 0.5320 - acc: 1.0000     \nEpoch 37/100\n3/3 [==============================] - 0s - loss: 0.5196 - acc: 0.6667     \nEpoch 38/100\n3/3 [==============================] - 0s - loss: 0.5088 - acc: 1.0000     \nEpoch 39/100\n3/3 [==============================] - 0s - loss: 0.5007 - acc: 1.0000     \nEpoch 40/100\n3/3 [==============================] - 0s - loss: 0.4971 - acc: 1.0000     \nEpoch 41/100\n3/3 [==============================] - 0s - loss: 0.4818 - acc: 1.0000     \nEpoch 42/100\n3/3 [==============================] - 0s - loss: 0.4729 - acc: 1.0000     \nEpoch 43/100\n3/3 [==============================] - 0s - loss: 0.4678 - acc: 1.0000     \nEpoch 44/100\n3/3 [==============================] - 0s - loss: 0.4582 - acc: 1.0000     \nEpoch 45/100\n3/3 [==============================] - 0s - loss: 0.4550 - acc: 1.0000     \nEpoch 46/100\n3/3 [==============================] - 0s - loss: 0.4432 - acc: 1.0000     \nEpoch 47/100\n3/3 [==============================] - 0s - loss: 0.4366 - acc: 1.0000     \nEpoch 48/100\n3/3 [==============================] - 0s - loss: 0.4312 - acc: 1.0000     \nEpoch 49/100\n3/3 [==============================] - 0s - loss: 0.4250 - acc: 1.0000     \nEpoch 50/100\n3/3 [==============================] - 0s - loss: 0.4158 - acc: 1.0000     \nEpoch 51/100\n3/3 [==============================] - 0s - loss: 0.4086 - acc: 1.0000     \nEpoch 52/100\n3/3 [==============================] - 0s - loss: 0.4052 - acc: 1.0000     \nEpoch 53/100\n3/3 [==============================] - 0s - loss: 0.4014 - acc: 1.0000     \nEpoch 54/100\n3/3 [==============================] - 0s - loss: 0.3943 - acc: 1.0000     \nEpoch 55/100\n3/3 [==============================] - 0s - loss: 0.3861 - acc: 1.0000     \nEpoch 56/100\n3/3 [==============================] - 0s - loss: 0.3810 - acc: 1.0000     \nEpoch 57/100\n3/3 [==============================] - 0s - loss: 0.3739 - acc: 1.0000     \nEpoch 58/100\n3/3 [==============================] - 0s - loss: 0.3754 - acc: 1.0000     \nEpoch 59/100\n3/3 [==============================] - 0s - loss: 0.3630 - acc: 1.0000     \nEpoch 60/100\n3/3 [==============================] - 0s - loss: 0.3594 - acc: 1.0000     \nEpoch 61/100\n3/3 [==============================] - 0s - loss: 0.3532 - acc: 1.0000     \nEpoch 62/100\n3/3 [==============================] - 0s - loss: 0.3465 - acc: 1.0000     \nEpoch 63/100\n3/3 [==============================] - 0s - loss: 0.3401 - acc: 1.0000     \nEpoch 64/100\n3/3 [==============================] - 0s - loss: 0.3354 - acc: 1.0000     \nEpoch 65/100\n3/3 [==============================] - 0s - loss: 0.3399 - acc: 1.0000     \nEpoch 66/100\n3/3 [==============================] - 0s - loss: 0.3266 - acc: 1.0000     \nEpoch 67/100\n3/3 [==============================] - 0s - loss: 0.3243 - acc: 1.0000     \nEpoch 68/100\n3/3 [==============================] - 0s - loss: 0.3228 - acc: 1.0000     \nEpoch 69/100\n3/3 [==============================] - 0s - loss: 0.3129 - acc: 1.0000     \nEpoch 70/100\n3/3 [==============================] - 0s - loss: 0.3110 - acc: 1.0000     \nEpoch 71/100\n3/3 [==============================] - 0s - loss: 0.3040 - acc: 1.0000     \nEpoch 72/100\n3/3 [==============================] - 0s - loss: 0.2963 - acc: 1.0000     \nEpoch 73/100\n3/3 [==============================] - 0s - loss: 0.2953 - acc: 1.0000     \nEpoch 74/100\n3/3 [==============================] - 0s - loss: 0.2918 - acc: 1.0000     \nEpoch 75/100\n3/3 [==============================] - 0s - loss: 0.2901 - acc: 1.0000     \nEpoch 76/100\n3/3 [==============================] - 0s - loss: 0.2805 - acc: 1.0000     \nEpoch 77/100\n3/3 [==============================] - 0s - loss: 0.2784 - acc: 1.0000     \nEpoch 78/100\n3/3 [==============================] - 0s - loss: 0.2730 - acc: 1.0000     \nEpoch 79/100\n3/3 [==============================] - 0s - loss: 0.2694 - acc: 1.0000     \nEpoch 80/100\n3/3 [==============================] - 0s - loss: 0.2620 - acc: 1.0000     \nEpoch 81/100\n3/3 [==============================] - 0s - loss: 0.2587 - acc: 1.0000     \nEpoch 82/100\n3/3 [==============================] - 0s - loss: 0.2548 - acc: 1.0000     \nEpoch 83/100\n3/3 [==============================] - 0s - loss: 0.2494 - acc: 1.0000     \nEpoch 84/100\n3/3 [==============================] - 0s - loss: 0.2485 - acc: 1.0000     \nEpoch 85/100\n3/3 [==============================] - 0s - loss: 0.2460 - acc: 1.0000     \nEpoch 86/100\n3/3 [==============================] - 0s - loss: 0.2414 - acc: 1.0000     \nEpoch 87/100\n3/3 [==============================] - 0s - loss: 0.2380 - acc: 1.0000     \nEpoch 88/100\n3/3 [==============================] - 0s - loss: 0.2316 - acc: 1.0000     \nEpoch 89/100\n3/3 [==============================] - 0s - loss: 0.2288 - acc: 1.0000     \nEpoch 90/100\n3/3 [==============================] - 0s - loss: 0.2253 - acc: 1.0000     \nEpoch 91/100\n3/3 [==============================] - 0s - loss: 0.2234 - acc: 1.0000     \nEpoch 92/100\n3/3 [==============================] - 0s - loss: 0.2196 - acc: 1.0000     \nEpoch 93/100\n3/3 [==============================] - 0s - loss: 0.2148 - acc: 1.0000     \nEpoch 94/100\n3/3 [==============================] - 0s - loss: 0.2119 - acc: 1.0000     \nEpoch 95/100\n3/3 [==============================] - 0s - loss: 0.2118 - acc: 1.0000     \nEpoch 96/100\n3/3 [==============================] - 0s - loss: 0.2076 - acc: 1.0000     \nEpoch 97/100\n3/3 [==============================] - 0s - loss: 0.2014 - acc: 1.0000     \nEpoch 98/100\n3/3 [==============================] - 0s - loss: 0.2003 - acc: 1.0000     \nEpoch 99/100\n3/3 [==============================] - 0s - loss: 0.1966 - acc: 1.0000     \nEpoch 100/100\n3/3 [==============================] - 0s - loss: 0.1930 - acc: 1.0000     \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "100 epochs puts out a lot of output. But by the time we get to the end you should see something like:\n\n `Epoch 100/100\n 3/3 [==============================] - 0s - loss: 0.1667 - acc: 1.0000`\n \nNotice our loss is 0.16 now, better but not very close to zero. Maybe your network trained to a lower loss. Go ahead and see how the percentages work out below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interact(getPredictionsFromModel, r = (0.0, 1.0, 0.01), g = (0.0, 1.0, 0.01), b = (0.0, 1.0, 0.01))",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed34edf7bf3e4693a56de731812a9ba9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "interactive(children=(FloatSlider(value=0.5, description='r', max=1.0, step=0.01), FloatSlider(value=0.5, desc…"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "<function __main__.getPredictionsFromModel>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "On my machine, I get a prediction of about 80% for gray, 85% when I set to black, and 92% when I set to white. Much better!\n\nBelow there's a number entry field that lets you rerun the training and test with whatever number of epochs you want. Try training to 50 or 200 epochs and see what results you get."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "@interact(epochs = (1, 500))\ndef trainModel(epochs=10):\n    global colorModel\n    global colorLabels\n    (colorModel, colorLabels) = train(colorMap, epochs=epochs)\n",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "860dcac7b1bf44e393322692e7e20840",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "interactive(children=(IntSlider(value=10, description='epochs', max=500, min=1), Output()), _dom_classes=('wid…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interact(getPredictionsFromModel, r = (0.0, 1.0, 0.01), g = (0.0, 1.0, 0.01), b = (0.0, 1.0, 0.01))",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97379bfaaf6247b4b3211589a6de1578",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "interactive(children=(FloatSlider(value=0.5, description='r', max=1.0, step=0.01), FloatSlider(value=0.5, desc…"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "<function __main__.getPredictionsFromModel>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "If you try out higher numbers like 300 or 400, you should see the loss getting down to 0.01, and when you run the sliders with that network the predictions get much more accurate.\n\n### Coming up...\nOur neural network has graduated to pre-kindergarten. Time to teach it more colors!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}